[global]

#per_job_logs=0
#log_avg_msec=50
#write_bw_log=hhhl_seqread.results
#write_iops_log=hhhl_iops.results
#write_lat_log=hhhl_lat.results


# bs= blocksize for test. Common sizes are 4k, 64k,128k,256k, 512k, 1M

bs=4k

# iodepth = queue depth. Common depths are 32, 64, 126, 256, 512
iodepth=64

# do not change direct value.
direct=1

# do not change ioengine
ioengine=libaio

# do not change randrepeat, this gives validation to test and makes sure you're not just writing on m.2 DRAM buffer stored data.
randrepeat=1

# reports all jobs into the same output. No need to change unless you think there's a single m.2 that's underperforming.
group_reporting

# sets the jobs to run as a time based run. Do not change.
time_based

# runtime is how long the test will run in seconds.
runtime=60

# filesize sets the size of the test file plot.
filesize=10G

# cpus_allowed sets what threads are allowed to be used for testing. Usage : Only use certain cpus by using CSV ie 0,1,2,3 .   Use a range of CPU ie 0-24.
cpus_allowed=0-16

# cpus_allowed_policy changes how the threads are locked. This can be changed to shared or split. Split dedicates one thread to one job
# shared will allow more than one job to run a thread. Tweak for performance.
cpus_allowed_policy=split

# numjobs sets the amount of threads to run per job. ie. if numbjobs=2 than each job will run two tests on the m.2. Change value to see where performance is the best. 
numjobs=2

# do not change invalidate. If set to 0 your test will not be valid.
invalidate=1

# ramp_time sets the amount of time to warm up the drives. Longer ramp times will fill the write buffer so you are able to log data for sustained writes only.
ramp_time=5

# How to use this job file:
# Job structure is broken down into four values, job number defined as [job$], job type as defined by rw=$$, 
# target for test is defined by filename=/dev/nvme$n$ and the name of the test as defined by name=$$=$$$.

#####JOB ASSIGNMENT SECTION#####

#This is where you will change the targets for testing and change the type of workload FIO will run.
#To change the target edit the lines "filename=/dev/nvmXn1".
#To change the type of workload. change "rw=XXX" to the available workloads listed below.
#Each job is broken up as "[jobX]" so you can run different workloads on different targets.
#If you are running the same workload on every drive you can comment out the "rw=XXX" in each job and uncomment the global setting below.

########################## !!!!!!!WARNING!!!!!!  IF YOU PERFORM A WRITE JOB YOU WILL DESTROY DATA! DOUBLE CHECK YOUR JOB TARGETS!  #####################

#### Uncomment this rw= line and set a global workload here. ########
rw=randread

# Remove comment to switch test mode.
# write = seq write
# read = seq read
# rw = seq read and write
# randread random reads
# randwrite = random writes
# randrw = random reads and writes
# trimwrite = before writing the block will be trimmed and then written. Probably won't ever be used.


[job1]

#rw=randread
filename=/dev/nvme0n1
name=raw=test

[job2]

#rw=randread
filename=/dev/nvme1n1
name=raw=test

[job3]

#rw=randread
filename=/dev/nvme2n1
name=raw=test

[job4]

#rw=randread
filename=/dev/nvme3n1
name=raw=test

[job5]

#rw=randread
filename=/dev/nvme4n1
name=raw=test

[job6]

#rw=randread
filename=/dev/nvme5n1
name=raw=test

[job7]

#rw=randread
filename=/dev/nvme6n1
name=raw=test

[job8]

#rw=randread
filename=/dev/nvme7n1
name=raw=test
